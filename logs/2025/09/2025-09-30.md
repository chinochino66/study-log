下記を完了済み。

# Kaggle Titanic - Machine Learning from Disaster

## 概要
Kaggle 入門コンペ「Titanic: Machine Learning from Disaster」に挑戦。  
データの前処理・特徴量エンジニアリング・複数モデルの比較を行い、最終的に **XGBoostでPublic LBスコア 0.751〜0.772** を達成。  

---

```python
# ==============================================
# Titanic: EDA + Preprocess + Model Comparison
# ==============================================

import os, re, datetime as dt
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

# ---- Paths ----
TRAIN_PATH = "/kaggle/input/titanic/train.csv"
TEST_PATH  = "/kaggle/input/titanic/test.csv"
OUT_DIR    = "/kaggle/working"
os.makedirs(OUT_DIR, exist_ok=True)

train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

# --------- 1) Feature Engineering ----------
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()

    # Title 抽出（Mr, Mrs, Miss, Master, Rare）
    out["Title"] = out["Name"].str.extract(r",\s*([^\.]+)\.", expand=False).fillna("Rare").str.strip()
    title_map = {
        "Mlle":"Miss","Ms":"Miss","Mme":"Mrs",
        "Lady":"Rare","Countess":"Rare","Capt":"Rare","Col":"Rare","Don":"Rare","Dr":"Rare",
        "Major":"Rare","Rev":"Rare","Sir":"Rare","Jonkheer":"Rare","Dona":"Rare"
    }
    out["Title"] = out["Title"].replace(title_map)
    out.loc[~out["Title"].isin(["Mr","Mrs","Miss","Master"]), "Title"] = "Rare"

    # 家族サイズ & 単独フラグ
    out["FamilySize"] = out["SibSp"].fillna(0) + out["Parch"].fillna(0) + 1
    out["IsAlone"] = (out["FamilySize"] == 1).astype(int)

    # 1人あたり運賃
    fare = out["Fare"].copy()
    fam = out["FamilySize"].replace(0, 1)
    out["FarePerPerson"] = (fare.fillna(fare.median())) / fam

    # CabinDeck（先頭文字）
    out["CabinDeck"] = out.get("Cabin", pd.Series(["U"]*len(out))).fillna("U").astype(str).str[0]
    out.loc[~out["CabinDeck"].isin(list("ABCDEFGT")+["U"]), "CabinDeck"] = "U"

    # TicketPrefix（数字以外）
    def ticket_prefix(x: str) -> str:
        if pd.isna(x): return "NONE"
        t = re.sub(r"\d", "", str(x)).replace(".", "").replace("/", "").strip()
        return t if t else "NONE"
    out["TicketPrefix"] = out.get("Ticket", pd.Series(["NONE"]*len(out))).map(ticket_prefix)

    # Age*Class の交互作用
    out["AgeTimesClass"] = out["Age"] * out["Pclass"]

    return out

train_fe = add_features(train)
test_fe  = add_features(test)
y = train_fe["Survived"].astype(int)

drop_cols = ["Survived","PassengerId","Name","Ticket","Cabin"]
features = [c for c in train_fe.columns if c not in drop_cols]

cat_cols = ["Sex","Embarked","Title","CabinDeck","TicketPrefix","Pclass"]
num_cols = [c for c in features if c not in cat_cols]

# --------- 2) 前処理パイプライン ----------
numeric_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])
categorical_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
])
preprocess = ColumnTransformer([
    ("num", numeric_tf, num_cols),
    ("cat", categorical_tf, cat_cols)
], remainder="drop")

# --------- 3) モデル比較 ----------
models = {
    "LogReg": LogisticRegression(max_iter=2000, random_state=42),
    "RF": RandomForestClassifier(n_estimators=600, random_state=42, n_jobs=-1),
    "GBDT": GradientBoostingClassifier(random_state=42),
    "XGB": XGBClassifier(
        n_estimators=600, max_depth=3, learning_rate=0.05,
        subsample=0.9, colsample_bytree=0.9,
        reg_lambda=1.0, random_state=42, n_jobs=-1,
        tree_method="hist", eval_metric="logloss"
    )
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = {}
for name, clf in models.items():
    pipe = Pipeline([("prep", preprocess), ("clf", clf)])
    scores = cross_val_score(pipe, train_fe[features], y, cv=cv, scoring="accuracy", n_jobs=-1)
    cv_scores[name] = (scores.mean(), scores.std())
    print(f"{name}: CV Acc = {scores.mean():.4f} ± {scores.std():.4f}")

best_name = max(cv_scores, key=lambda k: cv_scores[k][0])
print(f"\nBest model: {best_name} (CV Acc={cv_scores[best_name][0]:.4f})")

# --------- 4) 全データで再学習 → 提出 ----------
best_model = models[best_name]
best_pipe = Pipeline([("prep", preprocess), ("clf", best_model)])
best_pipe.fit(train_fe[features], y)

pred = best_pipe.predict(test_fe[features]).astype(int)
submission = pd.DataFrame({"PassengerId": test_fe["PassengerId"], "Survived": pred})

stamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
cvmean = f"{cv_scores[best_name][0]:.4f}".replace(".","p")
sub_path = f"{OUT_DIR}/submission_{best_name}_cv{cvmean}_{stamp}.csv"

submission.to_csv(sub_path, index=False)
print("Saved:", sub_path)
display(submission.head())
````

---

## 結果

* Logistic Regression: CV Acc ≈ 0.826
* Random Forest: CV Acc ≈ 0.823
* GBDT: CV Acc ≈ 0.834
* XGBoost: CV Acc ≈ **0.842 (ベスト)**

**Public LB スコア**

* Logistic Regression: 0.76076
* GBDT: 0.77272
* XGBoost: 0.75119

---

